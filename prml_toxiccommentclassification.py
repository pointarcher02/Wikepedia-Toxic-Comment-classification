# -*- coding: utf-8 -*-
"""PRML_ToxicCommentClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KbOoo7MyskEZB50iJJNMQoqCPNxQ59XK
"""

from google.colab import drive 
drive.mount('/content/drive')

"""# PRML Major Project

    Project Title : Toxic Comment Classification
    Team Members :  Shreshth Vatsal Sharma (B21CS094)
                    Shashank Shekhar Asthana (B21CS093)
                    Adeem Haris (B21AI004)

    
---
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os, sys
import time
import warnings
from sklearn.metrics import f1_score
from wordcloud import WordCloud
import re
import nltk
import string
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report,confusion_matrix

warnings.filterwarnings('ignore')

"""## Approach 1
    To classify each comment into 6 of the classes that we have and predicting the accuracy of our model
"""

#importing training and testing dataset

df_train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/train.csv')

df_test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/test.csv')

df_train

df_test.describe()

def want_unwant(df_train,label):
  vals1, counts1 = np.unique(df_train[label] , return_counts=True)
  return(vals1, counts1)

# A function that determines how much wanted and unwanted words are there in each label

labels = [ 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'] 
counts=[]
for i in labels:
  print('For',i,'the count is : ')
  vals,count = want_unwant(df_train,i)
  counts.append(count[1])
  print(want_unwant(df_train,i)[1])

"""    There are 15294 occurances of unwanted words in toxic label and 144277 occurances of wanted words in toxic label.
    There are 1595 occurances of unwanted words in sever_toxic label and 157976 occurances of wanted words in toxic label.
    There are 8449 occurances of unwanted words in obscene and 151122 occurances of wanted words in toxic label.
    There are 478 occurances of unwanted words in threat label and 159093 occurances of wanted words in toxic label.
    There are 7877 occurances of unwanted words in insult label and 151694 occurances of wanted words in toxic label.
    There are 1405 occurances of unwanted words in identity_hate label and 158166 occurances of wanted words in toxic label.
"""

counts

df_train['comment_text'][0]

words = ' '.join(df_train['comment_text'])
wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(words)

plt.figure(figsize=(10, 7))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis('off')
plt.show()

plt.figure(figsize=(10, 7))
plt.bar(labels, counts, color=['#0066ff', '#ff9900', '#ff0000', '#00cc00', '#6600ff', '#ff0066'])
plt.title('Class Distribution')
plt.xlabel('Labels')
plt.ylabel('Count')
plt.show()

def clean_text_data(text):
    text = text.lower()
    text = re.sub(r"what's", "what is ", text)
    text = re.sub(r"\'s", " ", text)
    text = re.sub(r"\'ve", " have ", text)
    text = re.sub(r"can't", "cannot ", text)
    text = re.sub(r"n't", " not ", text)
    text = re.sub(r"i'm", "i am ", text)
    text = re.sub(r"I'm", "I am ", text)
    text = re.sub(r"\'re", " are ", text)
    text = re.sub(r"\'d", " would ", text)
    text = re.sub(r"\'ll", " will ", text)
    text = re.sub(r"\'scuse", " excuse ", text)
    text = re.sub('\W', ' ', text)
    text = re.sub('\s+', ' ', text)
    text = re.sub('[0-9]', '', text)
    text = re.sub("\'", "", text)
    text = re.sub(r"\S*https?:\S*", " ", text)
    text = re.sub(r"\S*www.\S*", " ", text)
    text = text.strip(' ')
    return text

df_train['comment_text'] = df_train['comment_text'].apply(clean_text_data)
df_test['comment_text'] = df_test['comment_text'].apply(clean_text_data)
df_train['comment_text'][0]

df_train_new = df_train.drop(['id'],axis=1)

df_test

df_test_new = df_test.drop(['id'],axis=1)

df_train_new

X, y1, y2, y3, y4, y5, y6 = df_train_new['comment_text'], df_train_new['toxic'], df_train_new['severe_toxic'], df_train_new['obscene'], df_train_new['threat'], df_train_new['insult'], df_train_new['identity_hate']

print(X.shape, y1.shape, y2.shape, y3.shape, y4.shape, y5.shape, y6.shape)

X_train1, X_val1, y1_train, y1_val = train_test_split(X, y1, test_size=0.3, random_state=42)
X_train2, X_val2, y2_train, y2_val = train_test_split(X, y2, test_size=0.3, random_state=42)
X_train3, X_val3, y3_train, y3_val = train_test_split(X, y3, test_size=0.3, random_state=42)
X_train4, X_val4, y4_train, y4_val = train_test_split(X, y4, test_size=0.3, random_state=42)
X_train5, X_val5, y5_train, y5_val = train_test_split(X, y5, test_size=0.3, random_state=42)
X_train6, X_val6, y6_train, y6_val = train_test_split(X, y6, test_size=0.3, random_state=42)

X_train1

from sklearn.feature_extraction.text import TfidfVectorizer 

vectorizer = TfidfVectorizer(use_idf=True)

model = make_pipeline(vectorizer, LinearSVC())
model.fit(X_train1, y1_train)
pred_val1 = model.predict(X_val1)
score1 = f1_score(y1_val, pred_val1, average='weighted')
print(score1)

model.fit(X_train2, y2_train)
pred_val2 = model.predict(X_val2)
score2 = f1_score(y2_val, pred_val2, average='weighted')
print(score2)

model.fit(X_train3, y3_train)
pred_val3 = model.predict(X_val3)
score3 = f1_score(y3_val, pred_val3, average='weighted')
print(score3)

model.fit(X_train4, y4_train)
pred_val4 = model.predict(X_val4)
score4 = f1_score(y4_val, pred_val4, average='weighted')
print(score4)

model.fit(X_train5, y5_train)
pred_val5 = model.predict(X_val5)
score5 = f1_score(y5_val, pred_val5, average='weighted')
print(score5)

model.fit(X_train6, y6_train)
pred_val6 = model.predict(X_val6)
score6 = f1_score(y6_val, pred_val6, average='weighted')
print(score6)

df_test_new

df_test_sample = df_test.copy()

df_test_sample

df_test_label = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/test_labels.csv')
df_test_label.head()

df_test_label_modified = df_test_label.loc[df_test_label['toxic']==-1]

df_test_label_modified

df_test_label['comment_text'] = df_test_sample['comment_text']

df_test_label

df_test_modified = df_test_label.loc[df_test_label['toxic']!=-1]

df_test_modified

test_labels = df_test_modified.drop(columns=['comment_text'])

X_test_modified = df_test_modified.drop(columns=labels)

X_test_modified = X_test_modified.drop(columns = ['id'])

X_test_modified

y1_test_modified = df_test_modified['toxic']
y2_test_modified = df_test_modified['severe_toxic']
y3_test_modified = df_test_modified['obscene']
y4_test_modified = df_test_modified['threat']
y5_test_modified = df_test_modified['insult']
y6_test_modified = df_test_modified['identity_hate']

y1_test_modified.shape

X_test_modified['comment_text'] = X_test_modified['comment_text'].apply(clean_text_data)

model = make_pipeline(vectorizer, LinearSVC())
model.fit(X_train1, y1_train)
pred_test1 = model.predict(X_test_modified['comment_text'])
pred_test1.shape
score1_test = f1_score(y1_test_modified, pred_test1, average='weighted')
print(score1_test)

model = make_pipeline(vectorizer, LinearSVC())
model.fit(X_train2, y2_train)
pred_test2 = model.predict(X_test_modified['comment_text'])
score2_test = f1_score(y2_test_modified, pred_test2, average='weighted')
print(score2_test)

model = make_pipeline(vectorizer, LinearSVC())
model.fit(X_train3, y3_train)
pred_test3 = model.predict(X_test_modified['comment_text'])
score3_test = f1_score(y3_test_modified, pred_test3, average='weighted')
print(score3_test)

model = make_pipeline(vectorizer, LinearSVC())
model.fit(X_train4, y4_train)
pred_test4 = model.predict(X_test_modified['comment_text'])
score4_test = f1_score(y4_test_modified, pred_test4, average='weighted')
print(score4_test)

model = make_pipeline(vectorizer, LinearSVC())
model.fit(X_train5, y5_train)
pred_test5 = model.predict(X_test_modified['comment_text'])
score5_test = f1_score(y5_test_modified, pred_test5, average='weighted')
print(score5_test)

model = make_pipeline(vectorizer, LinearSVC())
model.fit(X_train6, y6_train)
pred_test6 = model.predict(X_test_modified['comment_text'])
score6_test = f1_score(y6_test_modified, pred_test6, average='weighted')
print(score6_test)

X_pred = X_test_modified.copy()

X_pred['toxic'] = pred_test1
X_pred['severe_toxic'] = pred_test2
X_pred['obscene'] = pred_test3
X_pred['threat'] = pred_test4
X_pred['insult'] = pred_test5
X_pred['identity_hate'] = pred_test6

X_pred

X_pred_copy = X_pred.copy()
X_pred_copy = X_pred_copy.drop(columns=["comment_text"])
test_labels = test_labels.drop(columns=["id"])

c=0
for i in range(len(X_pred_copy)) :
  if(X_pred_copy.to_numpy()[i].all() == test_labels.to_numpy()[i].all()) :
    c+=1
print("Accuracy for the Test dataset : " , c/len(X_pred))

for j in range(1,7) :
  c=0
  for i in range(len(X_pred)) :
    if(X_pred.to_numpy()[i][j] == 1) : c+=1
  print("Probability for a comment to be {} : ".format(labels[j-1]) , c/len(X_pred))

"""## Approach 2
    To Predict the probability of each type of toxicity for each comment.
---
"""

df_train_prob = df_train.copy()
df_test_prob = df_test.copy()

df_train_prob

df_train_prob.shape

df_train_prob.set_index('id')

df_train_prob['text length'] = df_train_prob['comment_text'].apply(len)

sns.distplot(a=df_train_prob['text length'],bins=30)

df_train_prob.hist(column='text length', by='toxic', bins=50,figsize=(12,4))

df_train_prob.hist(column='text length', by='severe_toxic', bins=50,figsize=(12,4))

df_train_prob['comment_text'].isnull().sum()

df_train_prob.head()

tfidf_vec = TfidfVectorizer(max_df=0.7,stop_words='english')

X_prob = df_train_prob['comment_text']
y_prob = df_train_prob['toxic']

X_train_prob, X_test_prob, y_train_prob, y_test_prob = train_test_split(X_prob, y_prob, test_size=0.33, random_state=42)

X_train_vec = tfidf_vec.fit_transform(X_train_prob)
X_test_vec = tfidf_vec.transform(X_test_prob)

log_prob = LogisticRegression()
log_prob.fit(X_train_vec,y_train_prob)

predictions = log_prob.predict(X_test_vec)
print(confusion_matrix(y_test_prob,predictions))
print(classification_report(y_test_prob,predictions))

X_prob = df_train_prob['comment_text']
y_prob = df_train_prob['severe_toxic']

X_train_prob, X_test_prob, y_train_prob, y_test_prob = train_test_split(X_prob, y_prob, test_size=0.33, random_state=42)

X_train_vec = tfidf_vec.fit_transform(X_train_prob)
X_test_vec = tfidf_vec.transform(X_test_prob)

log_st_prob = LogisticRegression()
log_st_prob.fit(X_train_vec,y_train_prob)

predictions = log_st_prob.predict(X_test_vec)
print(confusion_matrix(y_test_prob,predictions))
print(classification_report(y_test_prob,predictions))

X_prob = df_train_prob['comment_text']
y_prob = df_train_prob['obscene']

X_train_prob, X_test_prob, y_train_prob, y_test_prob = train_test_split(X_prob, y_prob, test_size=0.33, random_state=42)

X_train_vec = tfidf_vec.fit_transform(X_train_prob)
X_test_vec = tfidf_vec.transform(X_test_prob)

log_obs_prob = LogisticRegression()
log_obs_prob.fit(X_train_vec,y_train_prob)

predictions = log_obs_prob.predict(X_test_vec)
print(confusion_matrix(y_test_prob,predictions))
print(classification_report(y_test_prob,predictions))

X_prob = df_train_prob['comment_text']
y_prob = df_train_prob['threat']

X_train_prob, X_test_prob, y_train_prob, y_test_prob = train_test_split(X_prob, y_prob, test_size=0.33, random_state=42)

X_train_vec = tfidf_vec.fit_transform(X_train_prob)
X_test_vec = tfidf_vec.transform(X_test_prob)

log_thr_prob = LogisticRegression()
log_thr_prob.fit(X_train_vec,y_train_prob)

predictions = log_thr_prob.predict(X_test_vec)
print(confusion_matrix(y_test_prob,predictions))
print(classification_report(y_test_prob,predictions))

X_prob = df_train_prob['comment_text']
y_prob = df_train_prob['insult']

X_train_prob, X_test_prob, y_train_prob, y_test_prob = train_test_split(X_prob, y_prob, test_size=0.33, random_state=42)

X_train_vec = tfidf_vec.fit_transform(X_train_prob)
X_test_vec = tfidf_vec.transform(X_test_prob)

log_ins_prob = LogisticRegression()
log_ins_prob.fit(X_train_vec,y_train_prob)

predictions = log_ins_prob.predict(X_test_vec)
print(confusion_matrix(y_test_prob,predictions))
print(classification_report(y_test_prob,predictions))

X_prob = df_train_prob['comment_text']
y_prob = df_train_prob['identity_hate']

X_train_prob, X_test_prob, y_train_prob, y_test_prob = train_test_split(X_prob, y_prob, test_size=0.33, random_state=42)

X_train_vec = tfidf_vec.fit_transform(X_train_prob)
X_test_vec = tfidf_vec.transform(X_test_prob)

log_ih_prob = LogisticRegression()
log_ih_prob.fit(X_train_vec,y_train_prob)

predictions = log_ih_prob.predict(X_test_vec)
print(confusion_matrix(y_test_prob,predictions))
print(classification_report(y_test_prob,predictions))

df_test_prob.head()

df_test1 = df_test_prob['comment_text']
df_test1_vec = tfidf_vec.transform(df_test1)

prob_toxic = log_prob.predict_proba(df_test1_vec)
prob_stoxic = log_st_prob.predict_proba(df_test1_vec)
prob_obscene = log_obs_prob.predict_proba(df_test1_vec)
prob_threat = log_thr_prob.predict_proba(df_test1_vec)
prob_insult = log_ins_prob.predict_proba(df_test1_vec)
prob_ihate = log_ih_prob.predict_proba(df_test1_vec)

df1 = pd.DataFrame(prob_toxic[:,1],columns=['toxic'])
df2 = pd.DataFrame(prob_stoxic[:,1],columns=['severe_toxic'])
df3 = pd.DataFrame(prob_obscene[:,1],columns=['obscene'])
df4 = pd.DataFrame(prob_threat[:,1],columns=['threat'])
df5 = pd.DataFrame(prob_insult[:,1],columns=['insult'])
df6 = pd.DataFrame(prob_ihate[:,1],columns=['identity_hate'])

df7 = pd.concat([df_test_prob['id'],df1,df2,df3,df4,df5,df6],axis=1)

df7.head()

df7.set_index('id',inplace=True)

df7.head() #Final prediction of probabilities of each type of toxicity labels in each comment